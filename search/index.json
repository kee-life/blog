[{"content":"可以按照下文步骤，将 Demo 部署到本地集群。\n# 总体目标 部署 Dubbo 应用到 Kubernetes Istio 自动注入 Envoy 并实现流量拦截 基于 Istio 规则进行流量治理 # 基本流程与工作原理 这个示例演示了如何将 Dubbo 开发的应用部署在 Istio 体系下，以实现 Envoy 对 Dubbo 服务的自动代理，示例总体架构如下图所示。\n完成示例将需要的步骤如下：\n创建一个 Dubbo 应用( dubbo-samples-mesh-k8s ) 构建容器镜像并推送到镜像仓库（ 本示例官方镜像 ） 分别部署 Dubbo Provider 与 Dubbo Consumer 到 Kubernetes 并验证 Envoy 代理注入成功 验证 Envoy 发现服务地址、正常拦截 RPC 流量并实现负载均衡 优化并配置健康检查流程 # 详细步骤 # 环境要求 请确保本地安装如下环境，以提供容器运行时、Kubernetes集群及访问工具\nDocker Minikube Kubectl Istio Kubens(optional) 通过以下命令启动本地 Kubernetes 集群\n1 minikube start 通过 kubectl 检查集群正常运行，且 kubectl 绑定到默认本地集群\n1 kubectl cluster-info # 前置条件 通过以下命令为示例项目创建独立的 Namespace dubbo-demo，同时开启 sidecar 自动注入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 初始化命名空间并开启sidecar自动注入 cat \u0026gt; \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: dubbo-demo labels: istio-injection: enabled EOF # 切换命名空间 kubens dubbo-demo # 部署到 Kubernetes # 部署 Provider 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 # 部署 Service cat \u0026gt; \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: dubbo-samples-mesh-provider namespace: dubbo-demo spec: type: ClusterIP sessionAffinity: None selector: app: dubbo-samples-mesh-provider ports: - name: grpc-tri port: 50052 targetPort: 50052 EOF # 部署 Deployment cat \u0026gt; \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: dubbo-samples-mesh-provider-v1 namespace: dubbo-demo spec: replicas: 2 selector: matchLabels: app: dubbo-samples-mesh-provider version: v1 template: metadata: labels: app: dubbo-samples-mesh-provider version: v1 annotations: # Prevent istio rewrite http probe sidecar.istio.io/rewriteAppHTTPProbers: \u0026#34;false\u0026#34; spec: containers: - name: server image: apache/dubbo-demo:dubbo-samples-mesh-provider-v1_0.0.1 imagePullPolicy: Always ports: - name: grpc-tri containerPort: 50052 protocol: TCP - name: http-health containerPort: 22222 protocol: TCP livenessProbe: httpGet: path: /live port: http-health initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 1 readinessProbe: httpGet: path: /ready port: http-health initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 2 startupProbe: httpGet: path: /startup port: http-health failureThreshold: 30 initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 2 EOF 以上命令创建了一个名为 dubbo-samples-mesh-provider 的 Service，注意这里的 service name 与项目中的 dubbo 应用名是一样的。\n接着 Deployment 部署了一个 2 副本的 pod 实例，至此 Provider 启动完成。\n可以通过如下命令检查启动日志。\n1 2 3 4 5 # 查看 pod 列表 kubectl get pods -l app=dubbo-samples-mesh-provider # 查看 pod 部署日志 kubectl logs your-pod-id 这时 pod 中应该有一个 dubbo provider 容器实例，同时还有一个 Envoy Sidecar 容器实例。\n# 部署 Consumer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # 部署 Service cat \u0026gt; \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: dubbo-samples-mesh-consumer namespace: dubbo-demo spec: type: ClusterIP sessionAffinity: None selector: app: dubbo-samples-mesh-consumer ports: - name: grpc-dubbo protocol: TCP port: 50052 targetPort: 50052 EOF # 部署 Deployment cat \u0026gt; \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: dubbo-samples-mesh-consumer namespace: dubbo-demo spec: replicas: 1 selector: matchLabels: app: dubbo-samples-mesh-consumer version: v1 template: metadata: labels: app: dubbo-samples-mesh-consumer version: v1 annotations: # Prevent istio rewrite http probe sidecar.istio.io/rewriteAppHTTPProbers: \u0026#34;false\u0026#34; spec: containers: - name: server image: apache/dubbo-demo:dubbo-samples-mesh-consumer_0.0.1 imagePullPolicy: Always ports: - name: grpc-tri containerPort: 50052 protocol: TCP - name: http-health containerPort: 22222 protocol: TCP env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace # This environment variable does not need to be configured by default. When the domain name suffix used inside k8s is artificially changed, it is only necessary to configure this #- name: CLUSTER_DOMAIN # value: cluster.local livenessProbe: httpGet: path: /live port: http-health initialDelaySeconds: 5 periodSeconds: 5 readinessProbe: httpGet: path: /ready port: http-health initialDelaySeconds: 5 periodSeconds: 5 startupProbe: httpGet: path: /startup port: http-health failureThreshold: 30 initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 2 EOF 部署 consumer 与 provider 是一样的，这里也保持了 K8S Service 与 Dubbo consumer application name(在 dubbo.properties 中定义) 一致： dubbo.application.name=dubbo-samples-mesh-consumer。\nDubbo Consumer 服务声明中还指定了消费的 Provider 服务（应用）名 @DubboReference(version = \u0026quot;1.0.0\u0026quot;, providedBy = \u0026quot;dubbo-samples-mesh-provider\u0026quot;, lazy = true)\n# 检查 Provider 和 Consumer 正常通信 继执行 3.3 步骤后， 检查启动日志，查看 consumer 完成对 provider 服务的消费。\n1 2 3 4 5 6 7 8 # 查看 pod 列表 kubectl get pods -l app=dubbo-samples-mesh-consumer # 查看 pod 部署日志 kubectl logs your-pod-id # 查看 pod isitio-proxy 日志 kubectl logs your-pod-id -c istio-proxy 可以看到 consumer pod 日志输出如下( Triple 协议被 Envoy 代理负载均衡):\n1 2 3 4 5 ==================== dubbo invoke 0 end ==================== [10/08/22 07:07:36:036 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.22:50052, client: 172.18.96.22, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 1 end ==================== [10/08/22 07:07:42:042 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; consumer istio-proxy 日志输出如下:\n1 2 3 4 [2022-07-15T05:35:14.418Z] \u0026#34;POST /org.apache.dubbo.samples.Greeter/greet HTTP/2\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 19 160 2 1 \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;6b8a5a03-5783-98bf-9bee-f93ea6e3d68e\u0026#34; \u0026#34;dubbo-samples-mesh-provider:50052\u0026#34; \u0026#34;172.17.0.4:50052\u0026#34; outbound|50052||dubbo-samples-mesh-provider.dubbo-demo.svc.cluster.local 172.17.0.7:52768 10.101.172.129:50052 172.17.0.7:38488 - default 可以看到 provider pod 日志输出如下:\n1 2 3 [10/08/22 07:08:47:047 UTC] tri-protocol-50052-thread-8 INFO impl.GreeterImpl: Server test dubbo tri mesh received greet request name: \u0026#34;service mesh\u0026#34; [10/08/22 07:08:57:057 UTC] tri-protocol-50052-thread-9 INFO impl.GreeterImpl: Server test dubbo tri mesh received greet request name: \u0026#34;service mesh\u0026#34; provider istio-proxy 日志输出如下:\n1 2 3 4 5 [2022-07-15T05:25:34.061Z] \u0026#34;POST /org.apache.dubbo.samples.Greeter/greet HTTP/2\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 19 162 1 1 \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; \u0026#34;201e6976-da10-96e1-8da7-ad032e58db47\u0026#34; \u0026#34;dubbo-samples-mesh-provider:50052\u0026#34; \u0026#34;172.17.0.10:50052\u0026#34; inbound|50052|| 127.0.0.6:47013 172.17.0.10:50052 172.17.0.7:60244 outbound_.50052_._.dubbo-samples-mesh-provider.dubbo-demo.svc.cluster.local default # Istio 流量治理 部署 v2 版本的 demo provider\n1 kubectl apply -f https://raw.githubusercontent.com/apache/dubbo-samples/master/dubbo-samples-mesh-k8s/deploy/provider/Deployment-v2.yml 设置 VirtualService 与 DestinationRule，观察流量按照 4:1 的比例分别被引导到 provider v1 与 provider v2 版本。\n1 kubectl apply -f https://raw.githubusercontent.com/apache/dubbo-samples/master/dubbo-samples-mesh-k8s/deploy/traffic/virtual-service.yml 从消费端日志输出中，观察流量分布效果如下图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ==================== dubbo invoke 100 end ==================== [10/08/22 07:15:58:058 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 101 end ==================== [10/08/22 07:16:03:003 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.22:50052, client: 172.18.96.22, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 102 end ==================== [10/08/22 07:16:08:008 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 103 end ==================== [10/08/22 07:16:13:013 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v2: 172.18.96.6:50052, client: 172.18.96.6, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 104 end ==================== [10/08/22 07:16:18:018 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.22:50052, client: 172.18.96.22, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 105 end ==================== [10/08/22 07:16:24:024 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 106 end ==================== [10/08/22 07:16:29:029 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.22:50052, client: 172.18.96.22, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 107 end ==================== [10/08/22 07:16:34:034 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 108 end ==================== [10/08/22 07:16:39:039 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.22:50052, client: 172.18.96.22, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; ==================== dubbo invoke 109 end ==================== [10/08/22 07:16:44:044 UTC] main INFO action.GreetingServiceConsumer: consumer Unary reply \u0026lt;-message: \u0026#34;hello,service mesh, response from provider-v1: 172.18.96.18:50052, client: 172.18.96.18, local: dubbo-samples-mesh-provider, remote: null, isProviderSide: true\u0026#34; 查看 dashboard Istio 官网查看 如何启动 dashboard。\n# 修改示例 注意项目存储路径一定是英文，否则 protobuf 编译失败。 以为应用开发与打包的指引说明。 修改 Dubbo Provider 配置 dubbo-provider.properties\n1 2 3 4 5 6 7 8 9 # provider dubbo.application.name=dubbo-samples-mesh-provider dubbo.application.metadataServicePort=20885 dubbo.registry.address=N/A dubbo.protocol.name=tri dubbo.protocol.port=50052 dubbo.application.qosEnable=true # 为了使 Kubernetes 集群能够正常访问到探针，需要开启 QOS 允许远程访问，此操作有可能带来安全风险，请仔细评估后再打开 dubbo.application.qosAcceptForeignIp=true 修改 Dubbo Consumer 配置 dubbo-consumer.properties\n1 2 3 4 5 6 7 8 9 10 11 12 # consumer dubbo.application.name=dubbo-samples-mesh-consumer dubbo.application.metadataServicePort=20885 dubbo.registry.address=N/A dubbo.protocol.name=tri dubbo.protocol.port=20880 dubbo.consumer.timeout=30000 dubbo.application.qosEnable=true # 为了使 Kubernetes 集群能够正常访问到探针，需要开启 QOS 允许远程访问，此操作有可能带来安全风险，请仔细评估后再打开 dubbo.application.qosAcceptForeignIp=true # 标记开启 mesh sidecar 代理模式 dubbo.consumer.meshEnable=true 完成代码修改后，通过项目提供的 Dockerfile 打包镜像\n1 2 # 打包并推送镜像 mvn compile jib:build Jib 插件会自动打包并发布镜像。注意，本地开发需将 jib 插件配置中的 docker registry 组织 dubboteam 改为自己有权限的组织（包括其他 kubernetes manifests 中的 dubboteam 也要修改，以确保 kubernetes 部署的是自己定制后的镜像），如遇到 jib 插件认证问题，请参考相应链接配置 docker registry 认证信息。 可以通过直接在命令行指定 mvn compile jib:build -Djib.to.auth.username=x -Djib.to.auth.password=x -Djib.from.auth.username=x -Djib.from.auth.username=x，或者使用 docker-credential-helper.\n# 常用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # dump current Envoy configs kubectl exec -it ${your pod id} -c istio-proxy curl http://127.0.0.1:15000/config_dump \u0026gt; config_dump # 进入 istio-proxy 容器 kubectl exec -it ${your pod id} -c istio-proxy -- /bin/bash # 查看容器日志 kubectl logs ${your pod id} -n ${your namespace} kubectl logs ${your pod id} -n ${your namespace} -c istio-proxy # 开启自动注入sidecar kubectl label namespace ${your namespace} istio-injection=enabled --overwrite # 关闭自动注入sidecar kubectl label namespace ${your namespace} istio-injection=disabled --overwrite ","date":"2023-01-06T11:34:00Z","image":"https://blog.elmle.cn/posts/dubbo-mesh/cover_hu88069865d8cc031fe1bb034ff9b8189d_243958_120x120_fill_q75_box_smart1.jpeg","permalink":"https://blog.elmle.cn/posts/dubbo-mesh/","title":"Dubbo demo using Istio"},{"content":" # Install oh-my-zsh with curl 1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34; # Enabling Plugins and Themes # zsh-autosuggestions 1 git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions # zsh-syntax-highlighting download 1 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting nano ~/.zshrc find plugins=(git)\nAppend zsh-autosuggestions \u0026amp; zsh-syntax-highlighting to plugins() like this\nplugins=(git zsh-autosuggestions zsh-syntax-highlighting)\n# powerlevel10k download 1 git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/themes/powerlevel10k edit ~/.zshrc, add ZSH_THEME=\u0026quot;powerlevel10k/powerlevel10k\u0026quot;\nreopen terminal to configure p10k theme (or run p10k configure)\n# Install Nerd Fonts (optional) download 1 2 mkdir -p ~/.local/share/fonts cd ~/.local/share/fonts \u0026amp;\u0026amp; curl -fLo \u0026#34;Droid Sans Mono for Powerline Nerd Font Complete.otf\u0026#34; https://github.com/ryanoasis/nerd-fonts/raw/master/patched-fonts/DroidSansMono/complete/Droid%20Sans%20Mono%20Nerd%20Font%20Complete.otf cache fonts 1 fc-cache -vf ~/.local/share/fonts/ check 1 fc-list | grep -i droid set font for terminal # Ref oh-my-zsh zsh-autosuggestions zsh-syntax-highlighting https://gist.github.com/kevin-smets/8568070 ","date":"2021-03-19T10:34:00Z","image":"https://blog.elmle.cn/posts/zsh/daveverwer_hu5c7d4a5c639474a6b114a6c6cb3d4af0_789913_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.elmle.cn/posts/zsh/","title":"configure zsh environment"},{"content":"The GitLab Docker images are monolithic images of GitLab running all the necessary services in a single container. If you instead want to install GitLab on Kubernetes, see GitLab Helm Charts.\nFind GitLab’s official Docker image at:\nGitLab Docker image in Docker Hub # Installation # Installation GitLab using Docker Engine You can fine tune these directories to meet your requirements. Once you’ve set up the GITLAB_HOME variable, you can run the image:\n1 2 3 4 5 6 7 8 9 10 11 export GITLAB_HOME=/data sudo docker run --detach \\ --hostname gitlab.example.com \\ --publish 443:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume $GITLAB_HOME/config:/etc/gitlab \\ --volume $GITLAB_HOME/logs:/var/log/gitlab \\ --volume $GITLAB_HOME/data:/var/opt/gitlab \\ gitlab/gitlab-ee:latest This will download and start a GitLab container and publish ports needed to access SSH, HTTP and HTTPS. All GitLab data will be stored as subdirectories of $GITLAB_HOME. The container will automatically restart after a system reboot.\nGitLab Docker images\n","date":"2020-10-15T19:11:54Z","permalink":"https://blog.elmle.cn/posts/gitlab-docker-setup/","title":"setup a gitlab code repository with docker"},{"content":"In this post, we will describe the process of preparing a development environment for sofa-mesh \u0026amp; sofa-mosn.\n# minikube # install minikube 1 2 3 4 mkdir -p $GOPATH/src/k8s.io git clone https://github.com/kubernetes/minikube.git $GOPATH/src/k8s.io/minikube cd $_ make build guide\n# start minikube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 #!/bin/bash # # start-minikube.sh # Copyright (C) 2019 liugang.zlg \u0026lt;liugang.zlg@antfin.com\u0026gt; # # minikube start script # # check parameter if [ $# -ne 1 ]; then echo \u0026#34;Usage: $0 BUILD_ID\u0026#34; exit 1 fi # constants MINIKUBE_CACHE_DIR=$HOME/.minikube/cache CURRENT_CONTEXT=minikube-$1 WORKING_DIR=`pwd`/target/$CURRENT_CONTEXT CURRENT_DRIVER=kvm2 KUBERNETES_VERSION=v1.12.4 REGISTRY_MIRROR=https://uzs7j6l5.mirror.aliyuncs.com # setup working dirctory function setUpDir { mkdir -p $WORKING_DIR/.kube $WORKING_DIR/.minikube/cache/iso # cache cp -rf $MINIKUBE_CACHE_DIR/$KUBERNETES_VERSION $WORKING_DIR/.minikube/cache/ # cp $MINIKUBE_CACHE_DIR/iso/minikube-`minikube version|awk \u0026#39;{print $NF}\u0026#39;`.iso $WORKING_DIR/.minikube/cache/iso/ } # create minikube function setUpMiniKube() { export KUBECONFIG=$WORKING_DIR/.kube/config export MINIKUBE_HOME=$WORKING_DIR # config cache minikube config set cache \u0026#39;istio/proxyv2:1.1.0-snapshot.4,ubuntu:xenial,istionightly/base_debug:latest,prom/prometheus:v2.3.1\u0026#39; # 启动 minikube # --registry-mirror: docker.io 在国内访问困难，我们给 docker daemon 指定一个镜像 # --kubernetes-version: minikube 支持的 k8s 版本非常丰富，可使用命令 minikube get-k8s-versions 获取所有支持的版本 # --profile: 这个参数非常重要，正是它支撑一台宿主机起多个 minikube实例 # --vm-driver: 指定虚拟机 driver # --bootstrapper: 官方推荐使用 kubeadm 启动 k8s minikube start --memory=8192 --cpus=4 --disk-size=30g \\ --registry-mirror=$REGISTRY_MIRROR \\ --kubernetes-version=$KUBERNETES_VERSION \\ --profile=$CURRENT_CONTEXT \\ --vm-driver=$CURRENT_DRIVER \\ --bootstrapper=kubeadm \\ --docker-opt \u0026#34;bip=192.168.4.1/24\u0026#34; \\ --iso-url \u0026#34;file://$MINIKUBE_CACHE_DIR/iso/minikube-`minikube version|awk \u0026#39;{print $NF}\u0026#39;`.iso\u0026#34; \\ -v 5 # --cache-images \\ minikube profile $CURRENT_CONTEXT # 实践中发现 k8s 集群有时候会设置 master node 不允许调度，我们把污点去除 # kubectl taint nodes --all node-role.kubernetes.io/master- } # print minikube info function echoMiniKube() { echo \u0026#39;minikube ip\u0026#39;:`minikube ip` echo \u0026#39;minikube status:\u0026#39;`minikube status` echo \u0026#39;minikube config get profile:\u0026#39;`minikube config get profile` echo \u0026#39;which kubectl:\u0026#39;`which kubectl` echo \u0026#39;kubectl cluster-info:\u0026#39;`kubectl cluster-info` } # setup minikube environment function setUpEnv() { cat \u0026gt; $WORKING_DIR/minikube-env \u0026lt;\u0026lt; EOF export MINIKUBE_HOME=\u0026#34;$WORKING_DIR\u0026#34; export KUBECONFIG=\u0026#34;$WORKING_DIR/.kube/config\u0026#34; eval \\$(minikube docker-env) source \u0026lt;(kubectl completion bash) EOF source $WORKING_DIR/minikube-env } setUpDir setUpMiniKube echoMiniKube setUpEnv for i in {1..150}; do kubectl get po \u0026amp;\u0026gt; /dev/null if [ $? -ne 1 ]; then break fi sleep 2 done ","date":"2019-02-20T14:00:00Z","permalink":"https://blog.elmle.cn/posts/prepare-istio-env/","title":"prepare a devlopment environment for istio"},{"content":"Kubernetes 近几年很热门，在各大技术论坛上被炒的很火。它提供了强大的容器编排能力，与此同时 DevOps 的概念也来到大家身边，它模糊了开发和运维之间的边界，让大家更容易的从开发到运维一个大型的分布式系统。 本文会以初学者的视角，希望能让读者更好地理解 Kubernetes 出现的背景、技术架构和设计理念，其中会涉及到部分实现原理。\n# 背景 # Docker Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化，Docker 的实现主要依赖于 Linux 的 namespace、cgroups 和 UnionFS。 Docker 容器是完全使用沙箱机制，相互之间不会有任何接口。通过 Docker，实现进程、网络、挂载点和文件隔离，更好地利用宿主机资源。Docker 的强大到不需要关心宿主机的依赖，所有的一切都可以在镜像构建时完成，这也是 Docker 目前成为容器技术标准的原因。所以我们能看到在 Kubernetes 中 使用 Docker 作为容器（也支持 rkt）。\n# 微服务 随着软件的规模越来越大，业务模式越来越复杂，用户量的上升、地区的分布、系统性能的苛刻要求促成服务架构的从单体变成 SOA 再到如今的微服务，未来还可能演变为 Service Mesh ，Serverless等等。 如今，一个完整的后端系统不再是单体应用架构了，伴随着多年前的 DDD 概念重新回到大家的视线中。现在的系统被不同的职责和功能被拆成多个服务，服务之间复杂的关系以及单机的单点性能瓶颈让部署和运维变得很复杂，所以部署和运维大型分布式系统的需求急迫待解决。\n# Kubernetes 铺垫了这么多，终于说到本文的主角了。说 Kubernetes 之前，不得不提 Compose 和 Swarm，其实在 Kubernetes 还未一统江湖之前，这两个已经能实现大部分容器编排的能力了。但是在真正的大型系统上，它们却远远不如 Kubernetes。 在容器化和微服务时代，服务越来越多，容器个数也越来越多。Docker 如它 Logo 所示一样，一只只鲸鱼在大海里自由地游荡，而 Kubernetes 就像一个掌舵的船长，带着它们，有序的管理它们，这个过程其实就是容器编排。\nKubernetes 起源于 Google，很多设计都是源自于 Borg，是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效，并且提供了应用部署，规划，更新，维护的一种机制。\n# 小结 至此，读者了解了 Kubernetes 的前世今生，它的出现使 DevOps 更加火热，普通的开发工程师也能做很多运维操作了。\n# 设计理念 这一部分，我们会围绕 Kubernetes 的四个设计理念看看这些做法能给我们带来什么。\n# 申明式VS命令式 声明式和命令式是截然不同的两种编程方式，在命令式 API 中，我们可以直接发出服务器要执行的命令，例如： “运行容器”、“停止容器”等；在声明式 API 中，我们声明系统要执行的操作，系统将不断向该状态驱动。\n我们常用的 SQL 就是一种声明式语言，告诉数据库想要的结果集，数据库会帮我们设计获取这个结果集的执行路径，并返回结果集。众所周知，使用 SQL 语言获取数据，要比自行编写处理过程去获取数据容易的多。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.2.1 env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 我们来看看相同设计的 YAML，利用它，我们可以告诉 Kubernetes 最终想要的是什么，然后 Kubernetes 会完成目标。\n声明式 API 使系统更加健壮，在分布式系统中，任何组件都可能随时出现故障。当组件恢复时，需要弄清楚要做什么，使用命令式 API 时，处理起来就很棘手。但是使用声明式 API ，组件只需查看 API 服务器的当前状态，即可确定它需要执行的操作。\n# 显示的API Kubernetes 是透明的，它没有隐藏的内部 API。换句话说 Kubernetes 系统内部用来交互的 API 和我们用来与 Kubernetes 交互的 API 相同。\n这样做的好处是，当 Kubernetes 默认的组件无法满足我们的需求时，我们可以利用已有的 API 实现我们自定义的特性。\n# 无侵入性 感谢 Docker 容器技术的流行，使得 Kubernetes 为大家提供了无缝的使用方式。在容器化的时代，我们的应用达到镜像后，不需要改动就可以遨游在 Kubernetes 集群中。\nKubernetes 还提供存储 Secret、Configuration 等包含但不局限于密码、证书、容器镜像信息、应用启动参数能力。如此，Kubernetes 以一种友好的方式将这些东西注入 Pod，减少了大家的工作量，而无需重写或者很大幅度改变原有的应用代码。\n# 有状态的移植 在有状态的存储场景下，Kubernetes 如何做到对于服务和存储的分离呢？假设一个大型分布式系统使用了多家云厂商的存储方案，如何做到开发者无感于底层的存储技术体系，并且做到方便的移植？\n为了实现这一目标，Kubernetes 引入了 PersistentVolumeClaim（PVC）和 PersistentVolume（PV）API 对象。这些对象将存储实现与存储使用分离。\nPersistentVolumeClaim 对象用作用户以与实现无关的方式请求存储的方法，通过它来抹除对底层 PersistentVolume 的差异性。这样就使 Kubernetes 拥有了跨集群的移植能力。\n# 架构 首先要提及的是 Kubernetes 使用很具代表性的 C/S 架构方式，Client 可以使用 kubectl 命令行或者 RESTful 接口与 Kubernetes 集群进行交互。下面这张图是从宏观上看 Kubernetes 的整体架构，每一个 Kubernetes 集群都由 Master 节点 和 很多的 Node 节点组成。 # Master Master 是 Kubernetes 集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有 Etcd 存储服务，运行 API Server 进程，Controller Manager 服务进程及 Scheduler 服务进程，关联工作节点 Node。\nKubernetes API Server 提供 HTTP Rest 接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口。也是集群控制的入口进程； Kubernetes Controller Manager 是 Kubernetes 所有资源对象的自动化控制中心，它驱使集群向着我们所需要的最终目的状态； Kubernetes Schedule是 负责 Pod 调度的进程。\n# Node Node 是 Kubernetes 集群架构中运行 Pod 的服务节点。Node 是 Kubernetes 集群操作的单元，用来承载被分配 Pod 的运行，是 Pod 运行的宿主机。关联 Master 管理节点，拥有名称和 IP、系统资源信息。运行 Docker Runtime、kubelet 和 kube-proxy。\nkubelet 负责对Pod对于的容器的创建、启停等任务，发送宿主机当前状态； kube-proxy 实现 Kubernetes Service 的通信与负载均衡机制的重要组件； Docker Runtime 负责本机容器的创建和管理工作。\n# 实现原理 为了尽可能的让读者能明白 Kubernetes 是如何运作的，这里不会涉及到具体的细节实现，如有读者感兴趣可以自行参阅官网文档。这里以一个简单的应用部署示例来阐述一些概念和原理。\n# 创建 Kubernetes 集群 介绍架构的时候我们知道，Kubernetes 集群由 Master 和 Node 组成。\nMaster 管理集群的所有行为例如：应用调度、改变应用的状态，扩缩容，更新/降级应用等。\nNode 可以是是一个虚拟机或者物理机，它是应用的“逻辑主机”，每一个 Node 拥有一个 Kubelet，Kubelet 负责管理 Node 节点与 Master 节点的交互，同时 Node 还需要有容器操作的能力，比如 Docker 或者 rkt。理论上来说，一个 Kubernetes 为了应对生产环境的流量，最少部署3个 Node 节点。\n当我们需要在 Kubernetes 上部署应用时，我们告诉 Master 节点，Master 会调度容器跑在合适的 Node 节点上。\n我们可以使用 Minikube 在本地搭一个单 Node 的 Kubernetes 集群。\n# 部署应用 当创建好一个 Kubernetes 集群后，就可以把容器化的应用跑在上面了。我们需要创建一个 Deployment，它会告诉 Kubernetes Master 如何去创建应用，也可以来更新应用。\n当应用实例创建后，Deployment 会不断的观察这些实例，如果 Node 上的 Pod 挂了，Deployment 会自动创建新的实例并且替换它。相比传统脚本运维的方式，这种方式更加优雅。\n我们能通过 kubectl 命令或者 YAML 文件来创建 Deployment，在创建的时候需要指定应用镜像和要跑的实例个数，之后 Kubernetes 会自动帮我们处理。\n# 查看Pods 和 Nodes 下面来介绍下 Pod 和 Node： 当我们创建好 Deployment 的时候，Kubernetes 会自动创建 Pod 来承载应用实例。Pod 是一个抽象的概念，像一个“逻辑主机”，它代表一组应用容器的集合，这些应用容器共享资源，包括存储，网络和相同的内部集群 IP。\n任何一个 Pod 都需要跑在一个 Node 节点上。Node 是一个“虚拟机器”，它可以是虚拟机也可以是物理机，一个 Node 可以有多个 Pods，Kubernetes 会自动调度 Pod 到合适的 Node 上。\n# Service 与 LabelSelector Pods 终有一死，也就是说 Pods 也有自己的生命周期，当一个 Pod 挂了的时候，ReplicaSet 会创建新的，并且调度到合适的 Node 节点上。考虑下访问的问题，Pod 替换伴随着 IP 的变化，对于访问者来说，变化的 IP 是合理的；并且当有多个 Pod 节点时，如何 SLB 访问也是个问题，Service 就是为了解决这些问题的。 Service 是一个抽象的概念，它定义了一组逻辑 Pods，并且提供访问它们的策略。和其他对象一样，Service 也能通过 kubectl 或者 YAML 创建。Service 定义的 Pod 可以写在 LabelSelector 选项中（下文会介绍），也存在不指定 Pods 的情况，这种比较复杂，感兴趣的读者可以自行查阅资料。\nService 有以下几种类型：\nClusterIP（默认）：在集群中内部IP上暴露服务，此类型使Service只能从群集中访问； NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 :，可以从集群的外部访问一个 NodePort 服务； LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务； ExternalName：通过返回 CNAME 和它的值，（适用于外部 DNS 的场景） Labels 和 Selectors 能够让 Kubernetes 拥有逻辑运算的能力，有点像 SQL。举个例子：可以查找 app=hello_word 的所有对象，也可以查找 app in (a,b,c) abc的所有对象。\nLabels是一个绑定在对象上的 K/V 结构，它可以在创建或者之后的时候的定义，在任何时候都可以改变。\n# 应用扩用 前文提到我们可以使用 Deployment 增加实例个数，下图是原始的集群状态： 我们可以随意的更改 replicas （实例个数）来扩容，当我们更改了 Deployment 中的 replicas 值时，Kubernetes 会自动帮我们达到想要的目标实例个数，如下图： # 更新应用 更新应用和扩容类似，我们可以更改 Deployment 中的容器镜像，然后 Kubernetes 会帮住我们应用更新（蓝绿、金丝雀等方式），通过此功能，我们还可以实现切换应用环境、回滚、不停机CCD。下面是部署的过程，需要注意的是，我们可以指定新创建的 Pod 最大个数和不可用 Pod 最大个数： # 总结 到了最后，大家对 Kubernetes 能有个大概的了解了，但 Kubernetes 远远不止本文所介绍的这些内容。\n","date":"2019-01-25T11:00:00Z","permalink":"https://blog.elmle.cn/posts/talk-about-kubernetes/","title":"talk about kubernetes"},{"content":"Prepare a environent with golang, docker-ce, minikube installed. We will begin develop istio in this environment.\n# Install go language 1 2 3 4 5 6 7 8 # install older golang, because there must be go command during compile golang source code sudo yum install -y golang # get golang source mkdir -p ~/git \u0026amp;\u0026amp; git clone https://github.com/golang/go.git ~/git/go # compire source code cd ~/git/go/src; ./all.bash # remove older golang sudo yum remove -y golang # Install Docker-CE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Step 1: add aliyun and docker-ce repository sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache # Step 2: install docker-ce (we should set http(s)_proxy because of GFW) sudo yum install -y docker-ce # Step 3: start docker daemon sudo systemctl enable docker \u0026amp;\u0026amp; sudo systemctl start docker # Step 4: add current user to docker group sudo usermod -a -G docker $(whoami) \u0026amp;\u0026amp; newgrp docker # (Optional) Step 5: test docker docker run hello-world # Install docker-machine 1 2 3 base=https://github.com/docker/machine/releases/download/v0.16.1 \u0026amp;\u0026amp; curl -L $base/docker-machine-$(uname -s)-$(uname -m) \u0026gt;/tmp/docker-machine \u0026amp;\u0026amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine # Install VirtualBox(https://www.virtualbox.org/wiki/Linux_Downloads) doc\n1 2 3 4 5 6 7 8 9 10 11 # Step 1: download virtualbox curl -Lo VirtualBox-6.0-6.0.2_128162_el7-1.x86_64.rpm https://download.virtualbox.org/virtualbox/6.0.2/VirtualBox-6.0-6.0.2_128162_el7-1.x86_64.rpm # Step 2: install virtualbox sudo yum install VirtualBox-6.0-6.0.2_128162_el7-1.x86_64.rpm # Step 3: install vboxdrv kernel module sudo /usr/lib/virtualbox/vboxdrv.sh setup # Step 3: test create a virtual machine docker-machine create -d virtualbox --virtualbox-disk-size \u0026#34;100000\u0026#34; large # Install kvm driver 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # CentOS environment sudo yum install qemu-kvm qemu-img virt-manager libvirt libvirt-python libvirt-client virt-install virt-viewer bridge-utils libguestfs-tools sudo usermod -a -G libvirt $(whoami) newgrp libvirt # Ubuntu 16.04 sudo apt install qemu-kvm libvirt-bin sudo usermod -a -G libvirtd $(whoami) newgrp libvirtd # ubuntu 18.04 sudo apt install libvirt-clients libvirt-daemon-system qemu-kvm sudo usermod -a -G libvirt $(whoami) newgrp libvirt # kvm2 driver curl -Lo docker-machine-driver-kvm2 https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 \\ \u0026amp;\u0026amp; chmod +x docker-machine-driver-kvm2 \\ \u0026amp;\u0026amp; sudo cp docker-machine-driver-kvm2 /usr/local/bin/ \\ \u0026amp;\u0026amp; rm docker-machine-driver-kvm2 # list virtual machine virsh list --all # Install minikube We will install minikube in virturebox\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Step 1: download minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo cp minikube /usr/local/bin/ \u0026amp;\u0026amp; rm minikube # Step 2: download kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo cp kubectl /usr/local/bin/ \u0026amp;\u0026amp; rm kubectl # Step 3: start minikube REGISTRY_MIRROR=https://docker.mirrors.ustc.edu.cn KUBERNETES_VERSION=v1.12.4 CURRENT_CONTEXT=minikube-`date +%s` CURRENT_DRIVER=kvm2 minikube start --memory=8192 --cpus=4 --disk-size=30g \\ --registry-mirror=$REGISTRY_MIRROR \\ --kubernetes-version=$KUBERNETES_VERSION \\ --profile=$CURRENT_CONTEXT \\ --vm-driver=$CURRENT_DRIVER \\ --bootstrapper=kubeadm minikube profile $CURRENT_CONTEXT ","date":"2019-01-22T15:00:00Z","permalink":"https://blog.elmle.cn/posts/minikube-env-preparatioin/","title":"prepare a minikube environment"},{"content":"You must configure a remote that points to the upstream repository in Git to sync changes you make in a fork with the original repository. This also allows you to sync changes made in the original repository with the fork.\nOpen Terminal List the current configured remote 1 2 3 $ git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FOCK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FOCK.git (push) Specify a new remote upstream repository that will be synced with the fork 1 $ git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Verify the new upstream repository you\u0026rsquo;ve specified for your fork 1 2 3 4 5 $ git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master 1 2 3 4 5 6 7 $ git fetch upstream remote: Counting objects: 75, done. remote: Compressing objects: 100% (53/53), done. remote: Total 62 (delta 27), reused 44 (delta 9) Unpacking objects: 100% (62/62), done. From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY * [new branch] master -\u0026gt; upstream/master Check out your fork\u0026rsquo;s local master branch 1 2 $ git checkout master Switched to branch \u0026#39;master\u0026#39; Merge the changes from upstream/master into your local master branch. This brings your fork\u0026rsquo;s master branch into sync with the upstream repository, without losing your local changes 1 2 3 4 5 $ git merge upstream/master Updating 34e91da..16c56ad Fast-forward README.md | 5 +++-- 1 file changed, 3 insertions(+), 2 deletions(-) Push your changes 1 $ git push Configuring a remote for a fork Syncing a fork\n","date":"2018-06-05T19:11:54Z","permalink":"https://blog.elmle.cn/posts/configuring-a-remote-repository-for-a-fork/","title":"configuring a remote repository for a fork"},{"content":"gather and analyze packet traces for macOS\n","date":"2017-06-07T15:03:20Z","permalink":"https://blog.elmle.cn/posts/gather-and-analyze-packet-traces-for-macos/","title":"gather and analyze packet traces for macOS"}]